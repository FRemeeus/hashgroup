{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DataAugmentationProject_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FRemeeus/hashgroup/blob/main/DataAugmentationProject_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45tWrRDHoR1"
      },
      "source": [
        "# Download and extract dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URbOA2tTlRJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec5ebf60-1e2f-4025-8a91-ee9588d718a6"
      },
      "source": [
        "  # Mounting your google Drive\n",
        "from google.colab import drive  \n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Copying the dataset\n",
        "import time\n",
        "start = time.time()\n",
        "!cp -n /content/drive/MyDrive/dataset.zip .\n",
        "end = time.time()\n",
        "print(f\"Time consumed in copying the dataset: {round(end - start, 2)} seconds\")\n",
        "\n",
        "# Unzipping the dataset\n",
        "start = time.time()\n",
        "!unzip -nq /content/dataset\n",
        "end = time.time()\n",
        "print(f\"Time consumed in unzipping: {round(end - start, 2)} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Time consumed in copying the dataset: 9.38 seconds\n",
            "Time consumed in unzipping: 5.04 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck03Y13DHtxU"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XREU9JJZ4Fu7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abc2fae-5fc1-4baf-e944-0f1b9c03455a"
      },
      "source": [
        "from __future__ import print_function \n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt\n",
        "import pickle\n",
        "\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "seed = 42\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Version:  1.9.0+cu102\n",
            "Torchvision Version:  0.10.0+cu102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fcd192412f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnf7TwUqHz0N"
      },
      "source": [
        "# Choosing parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfcDttK24IoJ"
      },
      "source": [
        "# Top level data directory.\n",
        "data_dir = \"/content/dataset/\"\n",
        "\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet]\n",
        "model_name = \"resnet\"\n",
        "\n",
        "# Number of samples per class in training data\n",
        "num_train = 500\n",
        "\n",
        "# Number of samples per class in validation data\n",
        "num_val = 481\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 10\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 64\n",
        "\n",
        "# Number of epochs to train for \n",
        "num_epochs = 20\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model, \n",
        "# when True we only update the reshaped layer params\n",
        "feature_extract = False\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htBsExBsH5uF"
      },
      "source": [
        "# Code for loading models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwCy8xZIHHWR"
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
        "        input_size = 224\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    \n",
        "    return model_ft, input_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LibVTcVAIEi0"
      },
      "source": [
        "# Training script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_yyi6Za4ORA"
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    train_acc_log = []\n",
        "    val_acc_log = []\n",
        "\n",
        "    train_loss_log = []\n",
        "    val_loss_log = []\n",
        "\n",
        "    confusion_matrix = torch.zeros(num_classes, num_classes)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "\n",
        "            for i, data in enumerate(dataloaders[phase], 1):\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # creating confusion matrix <-- NEW PIECE OF CODE\n",
        "                    if phase == 'val':\n",
        "                        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
        "                            confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                \n",
        "                if i % 10 == 1:\n",
        "                    print(f\"Batch {i} / {len(dataloaders[phase])}, Loss: {loss.item()}\")\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # Storing results of this epoch\n",
        "            if phase == 'train':\n",
        "                train_acc_log.append(epoch_acc)\n",
        "                train_loss_log.append(epoch_loss)\n",
        "            if phase == 'val':\n",
        "                val_acc_log.append(epoch_acc)\n",
        "                val_loss_log.append(epoch_loss)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    return train_acc_log, train_loss_log, val_acc_log, val_loss_log, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5puvj9GLPez"
      },
      "source": [
        "# Initialize everything for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0qAC_a816_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe5f8863-b722-4fab-ba44-03bbdf45e258"
      },
      "source": [
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "# finetuning we will be updating all parameters. However, if we are \n",
        "# doing feature extract method, we will only update the parameters\n",
        "# that we have just initialized, i.e. the parameters with requires_grad\n",
        "# is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(params_to_update, lr=0.001)\n",
        "\n",
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t conv1.weight\n",
            "\t bn1.weight\n",
            "\t bn1.bias\n",
            "\t layer1.0.conv1.weight\n",
            "\t layer1.0.bn1.weight\n",
            "\t layer1.0.bn1.bias\n",
            "\t layer1.0.conv2.weight\n",
            "\t layer1.0.bn2.weight\n",
            "\t layer1.0.bn2.bias\n",
            "\t layer1.1.conv1.weight\n",
            "\t layer1.1.bn1.weight\n",
            "\t layer1.1.bn1.bias\n",
            "\t layer1.1.conv2.weight\n",
            "\t layer1.1.bn2.weight\n",
            "\t layer1.1.bn2.bias\n",
            "\t layer2.0.conv1.weight\n",
            "\t layer2.0.bn1.weight\n",
            "\t layer2.0.bn1.bias\n",
            "\t layer2.0.conv2.weight\n",
            "\t layer2.0.bn2.weight\n",
            "\t layer2.0.bn2.bias\n",
            "\t layer2.0.downsample.0.weight\n",
            "\t layer2.0.downsample.1.weight\n",
            "\t layer2.0.downsample.1.bias\n",
            "\t layer2.1.conv1.weight\n",
            "\t layer2.1.bn1.weight\n",
            "\t layer2.1.bn1.bias\n",
            "\t layer2.1.conv2.weight\n",
            "\t layer2.1.bn2.weight\n",
            "\t layer2.1.bn2.bias\n",
            "\t layer3.0.conv1.weight\n",
            "\t layer3.0.bn1.weight\n",
            "\t layer3.0.bn1.bias\n",
            "\t layer3.0.conv2.weight\n",
            "\t layer3.0.bn2.weight\n",
            "\t layer3.0.bn2.bias\n",
            "\t layer3.0.downsample.0.weight\n",
            "\t layer3.0.downsample.1.weight\n",
            "\t layer3.0.downsample.1.bias\n",
            "\t layer3.1.conv1.weight\n",
            "\t layer3.1.bn1.weight\n",
            "\t layer3.1.bn1.bias\n",
            "\t layer3.1.conv2.weight\n",
            "\t layer3.1.bn2.weight\n",
            "\t layer3.1.bn2.bias\n",
            "\t layer4.0.conv1.weight\n",
            "\t layer4.0.bn1.weight\n",
            "\t layer4.0.bn1.bias\n",
            "\t layer4.0.conv2.weight\n",
            "\t layer4.0.bn2.weight\n",
            "\t layer4.0.bn2.bias\n",
            "\t layer4.0.downsample.0.weight\n",
            "\t layer4.0.downsample.1.weight\n",
            "\t layer4.0.downsample.1.bias\n",
            "\t layer4.1.conv1.weight\n",
            "\t layer4.1.bn1.weight\n",
            "\t layer4.1.bn1.bias\n",
            "\t layer4.1.conv2.weight\n",
            "\t layer4.1.bn2.weight\n",
            "\t layer4.1.bn2.bias\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox9wzR9CIa0H"
      },
      "source": [
        "# Loading data and data augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c40RmKcwfPoJ"
      },
      "source": [
        "# Code to add black borders to an image to create a square image\n",
        "def get_pad(img):\n",
        "  retlist = np.zeros(2)\n",
        "  maxie = np.argmin(img.size)\n",
        "  retlist[maxie] = np.ceil((img.size[maxie-1]-img.size[maxie])/2)\n",
        "  return tuple(retlist.astype(int))\n",
        "\n",
        "class ResizePad():\n",
        "\n",
        "    def __init__(self, size, fill=0, padding_mode='constant'):\n",
        "      assert padding_mode in ['constant', 'edge', 'reflect', 'symmetric']\n",
        "      self.size = size\n",
        "      self.fill = fill\n",
        "      self.padding_mode = padding_mode\n",
        "\n",
        "    def __call__(self, img):\n",
        "      pad = transforms.functional.pad(img, get_pad(img), self.fill, self.padding_mode)\n",
        "      return transforms.CenterCrop(self.size)(transforms.functional.resize(pad, self.size))\n",
        "    \n",
        "    def __repr__(self):\n",
        "      return self.__class__.__name__ + '(fill={0}, padding_mode={1})'.\\\n",
        "            format(self.fill, self.padding_mode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpglNPoZITgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7471b9b-91f0-4a6f-b0a9-4d40a6675980"
      },
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "\n",
        "\"\"\"\n",
        "HIER JE AUGMENTATION NEER ZETTEN:\n",
        "bijvoorbeeld: custom_augmentation = transforms.RandomPerspective()\n",
        "kiezen uit: None, transforms.RandomPerspective(), transforms.RandomRotation(180), transforms.GaussianBlur(kernel_size=(5,5), sigma=(0.1, 1.0)), \n",
        "transforms.RandomVerticalFlip() en transforms.RandomHorizontalFlip().\n",
        "\"\"\"\n",
        "custom_augmentation = transforms.RandomErasing(p=0.5, scale=(0.02, 0.5), ratio=(0.3, 3.3), value='random')\n",
        "# custom_augmentation1 = ...\n",
        "# custom_augmentation2 = ...          <-- uncomment this you if want to stack augmentations\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "baseline_transform = transforms.Compose([\n",
        "  transforms.Resize((input_size, input_size)),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "data_transforms = {\n",
        "  'train': transforms.Compose([\n",
        "      transforms.Resize((input_size, input_size)),\n",
        "      transforms.ToTensor(),\n",
        "      custom_augmentation,\n",
        "      # custom_augmentation1,\n",
        "      # custom_augmentation2,         <-- uncomment this if you want to stack augmentations\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "  ]),\n",
        "  'val': baseline_transform\n",
        "}\n",
        "\n",
        "if not custom_augmentation:\n",
        "  title_label = 0\n",
        "elif str(custom_augmentation) == \"RandomPerspective(p=0.5)\":\n",
        "  title_label = 1\n",
        "elif str(custom_augmentation) == \"RandomRotation(degrees=[-180.0, 180.0], interpolation=nearest, expand=False, fill=0)\":\n",
        "  title_label = 2\n",
        "elif str(custom_augmentation) == \"GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 1.0))\":\n",
        "  title_label = 3\n",
        "elif str(custom_augmentation) == \"RandomVerticalFlip(p=0.5)\":\n",
        "  title_label = 4\n",
        "elif str(custom_augmentation) == \"RandomHorizontalFlip(p=0.5)\":\n",
        "  title_label = 5\n",
        "elif str(custom_augmentation) == \"ColorJitter(brightness=(1, 1.5), contrast=None, saturation=None, hue=None)\":\n",
        "  title_label = 6\n",
        "elif str(custom_augmentation) == \"RandomErasing()\":\n",
        "  title_label = 7\n",
        "\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create balanced train and validation sets\n",
        "def subset(dataset, samples_per_class):\n",
        "  idxs = []\n",
        "\n",
        "  last_target = -1\n",
        "  for i, target in enumerate(dataset.targets):\n",
        "    if target != last_target:\n",
        "      for j in range(samples_per_class):\n",
        "        idxs.append(i + j)\n",
        "    last_target = target\n",
        "\n",
        "  return torch.utils.data.Subset(dataset, idxs)\n",
        "\n",
        "val_data = subset(datasets.ImageFolder(os.path.join(data_dir, 'val'), data_transforms['val']), num_val)\n",
        "baseline = datasets.ImageFolder(os.path.join(data_dir, 'train'), baseline_transform)\n",
        "\n",
        "double = False\n",
        "if custom_augmentation:\n",
        "  if double:\n",
        "    add_on = datasets.ImageFolder(os.path.join(data_dir, 'train'), data_transforms['train'])\n",
        "    train_data = torch.utils.data.ConcatDataset([subset(baseline, num_train), subset(add_on, num_train)])\n",
        "  else:\n",
        "    train_data = subset(datasets.ImageFolder(os.path.join(data_dir, 'train'), data_transforms['train']), num_train)\n",
        "else:   \n",
        "  train_data = subset(baseline, num_train)    \n",
        "\n",
        "dataloaders_dict = {}\n",
        "dataloaders_dict['val'] = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=2, shuffle=True)\n",
        "dataloaders_dict['train'] = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomErasing(p=0.5, scale=(0.02, 0.5), ratio=(0.3, 3.3), value=random, inplace=False)\n",
            "Initializing Datasets and Dataloaders...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szFBI8u-LucX"
      },
      "source": [
        "# Train and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rnE_1rfqpGT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "2e89abb5-f822-4693-d79f-7fed8b4d5e10"
      },
      "source": [
        "train_acc_log, train_loss_log, val_acc_log, val_loss_log, confusion_matrix = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-03259ac453ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_acc_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQGoM0CJRA4c"
      },
      "source": [
        "# code to save the runs into a folder\n",
        "def list_to_cpu(tensor_list):\n",
        "  lst = []\n",
        "  for element in tensor_list:\n",
        "    lst.append(element.to(torch.device('cpu')))\n",
        "  return lst\n",
        "\n",
        "pickle.dump([list_to_cpu(train_acc_log), train_loss_log, list_to_cpu(val_acc_log), val_loss_log, confusion_matrix], \n",
        "            open(\"drive/MyDrive/roaddefects/log_param/\" + str(model_name) + \"§\" + str(custom_augmentation) + \"§\" + str(seed) + \".txt\", \"wb\" ))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}